\documentclass{article}
\pdfoutput=1
% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true,
            colorlinks=true,
            linkcolor=blue,
            citecolor=blue,
            filecolor=blue,
            urlcolor=blue}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{bbm}

\newcommand{\aki}[1]{\textcolor{red}{[Aki: #1]}}

\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\Dir}[0]{\textrm{Dirichlet}}
\newcommand{\Ray}[0]{\textrm{Rayleigh}}
\newcommand{\gam}[0]{\textrm{Gamma}}
\newcommand{\dgamma}[0]{\textrm{Gamma}}
\newcommand{\dpoisson}[0]{\textrm{Poisson}}
\newcommand{\dbeta}[0]{\textrm{Beta}}
\newcommand{\dbern}[0]{\textrm{Bernoulli}}
\newcommand{\dunif}[0]{\mathrm{Uniform}}
\newcommand{\dgig}[0]{\textrm{GIG}}
\newcommand{\dnormal}[0]{\mathrm{Normal}}
\newcommand{\dt}[0]{\mathrm{t}}
\newcommand{\igamma}[0]{\textrm{Gamma}^{-1}}
\newcommand{\rayl}[0]{\textrm{Rayleigh}}
\newcommand{\Exp}[0]{\textrm{Exponential}}
\newcommand{\Bet}[0]{\textrm{Beta}}
\newcommand{\GEM}[0]{\textrm{GEM}}
\newcommand{\DP}[0]{\textrm{DP}}
\newcommand{\ESS}[0]{\mathrm{ESS}}
\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bpi}{\bm{\pi}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bb}{\bm{b}}
\newcommand{\bk}{\bm{k}}
\newcommand{\bl}{\bm{l}}
\newcommand{\bn}{\bm{n}}
\newcommand{\bw}{\bm{w}}
\newcommand{\bz}{\bm{z}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bX}{\bm{X}}
\newcommand{\by}{\bm{y}}
\newcommand{\bZ}{\bm{Z}}
\newcommand{\bW}{\bm{W}}
\newcommand{\bS}{\bm{S}}
\newcommand{\bH}{\bm{H}}
\newcommand{\Mult}{\textrm{Multinomial}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\NEW}{\textrm{\tiny new}}
\newcommand{\OLD}{\textrm{\tiny old}}
\newcommand{\sigmat}{\sigma^2}
\newcommand{\IBP}{\textrm{IBP}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Eq}{\mathbb{E}_q}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cHq}{\mathcal{H}_q}
\newcommand{\test}[1]{\mbox{$#1$}^{\small \mbox{test}}}
\newcommand{\alphaW}{\alpha^{(W)}}
\newcommand{\alphaH}{\alpha^{(H)}}
\newcommand{\betaW}{\beta^{(W)}}
\newcommand{\betaH}{\beta^{(H)}}
\newcommand{\gammaW}{\gamma^{(W)}}
\newcommand{\gammaH}{\gamma^{(H)}}
\newcommand{\gammaT}{\gamma^{(\theta)}}
\newcommand{\rhoW}{\rho^{(W)}}
\newcommand{\rhoH}{\rho^{(H)}}
\newcommand{\rhoT}{\rho^{(\theta)}}
\newcommand{\tauW}{\tau^{(W)}}
\newcommand{\tauH}{\tau^{(H)}}
\newcommand{\tauT}{\tau^{(\theta)}}
\newcommand{\muW}{\hat{W}}
\newcommand{\muH}{\hat{H}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\LF}{\mathrm{Leapfrog}}

\title{$R^*$: A robust MCMC convergence diagnostic with uncertainty using gradient-boosted machines}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
	 Ben Lambert\\
	 MRC Centre for Global Infectious Disease Analysis\\
	 School of Public Health\\
	 Imperial College London\\
	 W2 1PG, United Kingdom\\
	 \texttt{ben.c.lambert@gmail.com} \\
	 \And
	 Aki Vehtari \\
	 Department of Computer Science\\
	 Aalto University\\
	 Finland\\
	 \texttt{aki.vehtari@aalto.fi}
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Markov chain Monte Carlo (MCMC) has transformed Bayesian model inference over the past three decades: mainly because of this, Bayesian inference is now a workhorse of applied scientists. Under general conditions, MCMC sampling converges asymptotically to the posterior distribution, but this provides no guarantees about its finite sample performance. The predominant method for monitoring convergence is to run multiple chains and monitor individual chains' characteristics and compare these to the population as a whole: if within-chain and between-chain summaries are comparable, then this is taken to indicate that the chains have converged to a common stationary distribution. Here, we introduce a new method for diagnosing convergence based on whether a machine learning classifier model can successfully discriminate the individual chains. We call this convergence measure $R^*$. In contrast to the predominant $\widehat{R}$, $R^*$ is a single statistic across all parameters that indicates lack of mixing, although individual variables' importance for this metric can also be determined. Additionally, $R^*$ is not based on any single characteristic of the sampling distribution; instead using all the information in the chain, including that given by the joint sampling distribution, which is currently largely overlooked by existing approaches. Since the used machine learning classifier, gradient-boosted regression trees (GBM), provides uncertainty in predictions, as a byproduct, we obtain uncertainty in $R^*$. The method is straightforward to implement, robust to GBM hyperparameter choice, and could be a complementary additional check on MCMC convergence for applied analyses.
\end{abstract}

\section{Introduction}
Markov chain Monte Carlo (MCMC) is the class of exact-approximate methods that has contributed most to applied Bayesian inference in recent years. In particular, MCMC has made Bayesian inference widely available to a diverse community of practitioners through the many software packages that use it as an internal inference engine: from Gibbs sampling \citep{geman1984stochastic}, which underpins the popular BUGS \citep{lunn2000winbugs} and JAGS \citep{plummer2003jags} libraries, to more recent algorithms: for example, Hamiltonian Monte Carlo (HMC) \citep{neal2011mcmc}, the No U-Turn Sampler (NUTS) \citep{hoffman2014no}, and a dynamic HMC variant \citep{betancourt2017conceptual}, which Stan \citep{carpenter2017stan} and PyMC3 \citep{salvatier2016probabilistic} implement. MCMC methods are currently the most effective tools for sampling from many classes of posterior distributions encountered in applied work, and it seems unlikely that this trend will change soon.

Its importance in applied scientists' toolkits means it is essential that MCMC is used properly and with adequate care. A cost of automated inference software is that it is increasingly easy to regard MCMC as oracular: giving uncompromised views onto the posterior. Because of this, software packages (Stan in particular \citep{carpenter2017stan} is a great exemplar of this), go to great lengths to communicate to users any issues with sampling.


The most important determination of whether MCMC has worked is whether the sampling distribution has converged to the posterior \citep{brooks2011handbook}. MCMC methods are thus created because of an asymptotic property: that given an infinite number of draws, their sampling distribution approaches the posterior (under general conditions). Although the guarantees are asymptotic, MCMC estimates can have negligible bias with only a relatively small finite number of draws.

The predominant diagnostic method for determining whether practical convergence has occurred relies on the fact that the posterior distribution is the unique stationary distribution for an MCMC sampler. Therefore, it would appear that, if an MCMC sampling distribution stops changing, then convergence has occurred. Unfortunately, anyone who uses MCMC knows that it is full of false dawns: chains can easily become stuck in areas of parameter space, and observation over short intervals mean the sampling distribution \textit{appears} converged. Like furious bees trapped in a room of a house \citep{lambertbees}, MCMC samplers may fail to move due to the narrow gaps that join neighbouring areas. With MCMC, absence of evidence of new areas of high posterior density is, time and again, not evidence of their absence.

To combat this curse of hindsight, running multiple, independent chains, which have been initialised at diverse areas of parameter space is recommended \citep{gelman1992inference}. If the chains appear not to ``mix'' -- a term essentially meaning that it is difficult to resolve an individual chain's path from the mass of paths overlaid on top of one another -- they are yet to converge. This approach makes it less likely that faux-convergence will occur due to chains becoming stuck in an area of parameter space, and running multiple chains is standard practice in applied inference \citep{lambert2018Student}. The predominant approach to quantitatively measuring this mixing is to compare each chain's sampling distribution to that of the population of chains as a whole: specifically, $\widehat{R}$ -- the main convergence statistic used -- compares within-chain variance to that between-chains \citep{gelman1992inference}. If these variances are similar, $\widehat{R}\approx 1$, and chains are deemed to have mixed. Recently, Stan has adopted more advanced variations on the original $\widehat{R}$ formula: for example, splitting individual chains in two to combat poor intra-chain mixing \citep{gelman2013bayesian}; and using ranks of parameter draws rather than the raw values themselves to calculate $\widehat{R}$ \citep{vehtari2019rank}. Additionally, there has been more focus on ensuring that the effective sample size (ESS), a measure of sample quality (see, for example, \citep{lambert2018Student}), is sufficient, and accordingly, new measures of this quantity have been proposed \citep{vehtari2019rank} and adopted \citep{carpenter2017stan}. Collectively, these statistics help alert users of MCMC to issues with sampling (that typically echo issues with the model) meaning that all is not hunky dory.

Here, we introduce $R^*$, a new convergence metric. This statistic is built on the intuition that, if chains are mixed, it should not be possible to discern from a draw's value the chain that generated it. Rephrased, it should not be possible to predict the chain that \textit{caused} a draw. In this vein, we use a machine learning (ML) classifier to measure convergence. Specifically, we train a ML classifier to predict the chain that generated each observation. By evaluating the performance of the classifier on a held-out test set, this provides a new convergence metric. To maximise predictive accuracy, our chosen ML classifier naturally exploits differences in the full joint distributions between the chains, which means it's sensitive to variations across the joint distribution of target model dimensions unlike most existent convergence diagnostics. Our statistic, unlike its $\widehat{R}$ cousins, is scalar valued for multivariate distributions: one model provides a single $R^*$, whereas $\widehat{R}$ has separate values for each univariate marginal distribution. However, the ML classifier we use can straightforwardly be interrogated to determine which parameters were most important for generating predictive accuracy. For our ML classifier, we use gradient-boosted regression trees \citep{friedman2001greedy,greenwell2019package} (``GBM''), since these are known to perform well for the types of tabular data that our problem presents \citep{chollet2018}. For the types of problem we test, $R^*$ calculation is of a speed comparable to some of the newer $\widehat{R}$ measures calculated (typically O(seconds) to calculate), although for models with 10,000s of parameters and many iterations, the time taken is longer. It is also insensitive to GBM's hyperparameters and provides a measure of convergence robust to various Markov chain pathologies. In addition, since GBMs can output predicted class probabilities, we obtain uncertainty measures for $R^*$, which we find provides a useful summary of MCMC convergence. $R^*$ can straightforwardly be incorporated into existing software libraries to provide a complementary convergence metric alongside more established measures.

The structure of this paper is as follows: in \S\ref{sec:method}, we describe in detail the method for calculating $R^*$ and its uncertainty; in \S\ref{sec:results}, we examine the performance of $R^*$ across a range of scenarios introduced in \citep{vehtari2019rank} and elsewhere. Code for reproducing the analyses is provided at \url{https://github.com/ben18785/ml-mcmc-convergence}.


\section{Method}\label{sec:method}
If Markov chains have not mixed, it is possible to determine to which chain a draw belongs from its value. This is possible if there are differences in the sampling distribution for any dimension, $\theta$, in the target distribution (Fig. \ref{fig:marginal}): in this case, if the marginal distributions differ between chains, this information can be used to predict which chain a draw belongs to. It  is also possible to predict the chain that generated a given draw if there are differences in the joint distribution of two (or more) dimensions of the target, even if the marginal distributions are the same (Fig. \ref{fig:joint}).

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/unmixed_1.pdf}}
	\caption{\textbf{Chain prediction based on the marginal distribution of a single parameter.} A shows the path of two chains that have mixed (to the right of panel); B shows two chains that have not mixed.}
	\label{fig:marginal}
\end{figure}

\begin{figure}[h]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/unmixed_2.pdf}}
	\caption{\textbf{Chain prediction based on the joint distribution of two parameters where each chain's marginals are the same.} A shows the path of two chains that have mixed resulting in similar sampling distributions (to the right and above each panel); B shows two chains that have not mixed.}
	\label{fig:joint}
\end{figure}

These two cases, whilst simple, illustrate the basis of our approach. To determine if a set of Markov chains has converged to the same distribution, we train a supervised machine learning (ML) model to classify the chain to which each draw belongs. By evaluating its performance on an independent test set, we delineate whether chains have mixed based on whether classification accuracy is above the ``null'' case, where accuracy is $1/{N}$, and $N$ is the number of chains. By taking the ratio of ML accuracy to this null accuracy, we obtain a statistic that is interpretable in a similar way to $\widehat{R}$ \citep{gelman2013bayesian}. In a nod to this established statistic, we call our statistic $R^*$, and, by design, $R^*\approx 1$ signifies convergence. Algorithm \ref{alg:R_star} gives a recipe for calculating $R^*$.

\begin{algorithm}[tb]
	\caption{$R^*$ calculation}
	\label{alg:R_star}
	\begin{algorithmic}
		\STATE Given chain-wise draws from the target, $\{X^{\{1\}},X^{\{2\}},...,X^{\{N\}}\}$ and a test set length, $S_\text{test}$:
		\FOR{$m=1$ to $N$}
		\STATE Create train and test sets by random-sampling (w/o replacement), $X^{\{m\}}\rightarrow\{X^{\{m\}}_\text{train},X^{\{m\}}_\text{test}\}$
		\ENDFOR
		\STATE Stack $X_\text{train} = (X^{\{1\}}_\text{train},X^{\{2\}}_\text{train},...,X^{\{N\}}_\text{train})^T$
		\STATE Stack $X_\text{test} = (X^{\{1\}}_\text{test},X^{\{2\}}_\text{test},...,X^{\{N\}}_\text{test})^T$
		\STATE Train ML model to classify chain id from any draw, $x$: $\text{ML}(x|X_\text{train}) \rightarrow c$
		\FOR{$s=1$ to $S_\text{test}$}
		\STATE Obtain test draw, $x^{\{s\}}=X_\text{test}(s)\in \mathbb{R}^K$
		\STATE Predict chain id, $c^{\{s\}} = \text{ML}(x^{\{s\}}|X_\text{train})$
		\STATE Compare with actual id, $c^s$: $a^{\{s\}}=\mathbbm{1}(c^{\{s\}}=c^s)$
		\ENDFOR
		\STATE Calculate predictive accuracy, $\bar{a} = \frac{1}{S_\text{test}} \sum_{s=1}^{S_\text{test}} a^{\{s\}}$
		\STATE Calculate ratio to null model accuracy, $R^* = \bar{a} / (1 / N) = N \bar{a}$
		\RETURN $R^*$
	\end{algorithmic}
\end{algorithm}

The ML classifier we use here is a gradient-boosted regression tree (also known as a type of gradient-boosted machine or GBM, introduced in \cite{friedman2001greedy}), which experience has dictated to be a highly predictive framework for use in tabular data \citep{chollet2018} like ours. Specifically, we use the GBM implementation in \textbf{\textsf{R}}'s ``Caret'' package \citep{kuhn2008building}, which, in turn, uses the ``gbm'' package \citep{greenwell2019package}. The data for each chain has dimensions: $X\in \mathbb{R}^{S}\times \mathbb{R}^{K}$, where $S$ is the number of draws taken (here assumed the same for each chain, but this is not a binding constraint) and $K$ is the number of parameters. We split each chain's draws into randomly divided training and testing tests: here, we use 70\% of draws for training and 30\% for testing. Our GBM model was rapid to execute training then prediction on the testing set (taking $<1$ second on a desktop computer for both these steps for most models we consider in \S\ref{sec:results}), and its predictive performance was insensitive to its hyperparameters. In all the examples explored in \S\ref{sec:results}, the GBM hyperparameter settings we used were: an interaction depth of 3, a shrinkage parameter of 0.1, 10 observations being the minimum required for each node, and that 50 trees would be grown (see \url{https://github.com/ben18785/ml-mcmc-convergence} to replicate this functionality).

We can also get class probabilities, which we leverage to produce an uncertainty distribution for $R^*$. Algorithm \ref{alg:R_star_uncertainty} gives a recipe for generating draws from this distribution, which we now elaborate on in words. For each draw, $s$, in our testing set, GBM outputs a simplex of chain probabilities: $\boldsymbol{p}^{\{s\}}=(p_1^{\{s\}},p_2^{\{s\}},...,p_N^{\{s\}})$, which forms a categorical distribution that can be sampled from to yield a unique chain prediction, $c^{\{s\}}$. By comparing this classification to the true classification, $c^s$, we obtain a binary measure, $a^{\{s\}}=\mathbbm{1}(c^{\{s\}}=c^s)$, of whether this prediction was correct. We repeat this process for each draw in the testing set, generating $\boldsymbol{a}=(a^{\{1\}},a^{\{2\}},...,a^{\{N_\text{test}\}})$, whose average yields a single $R^{*\{i\}}=N \bar{a}$ estimate for iteration $i$. We then iterate this process, for $i=1,2,...,I$, producing a set of $(R^{*\{1\}},R^{*\{2\}},...,R^{*\{I\}})$, which collectively represent a distribution for $R^*$. 


\begin{algorithm}[tb]
	\caption{Procedure to generate $I$ draws of $R^*$}
	\label{alg:R_star_uncertainty}
	\begin{algorithmic}
		\STATE Given test data $X_\text{test}$, number of chains $N$, number of iterations $I$, and fitted
		\STATE model, $\text{ML}(x|X_\text{train})\rightarrow(p_1,p_2,...,p_N)$:
		\FOR{$i=1$ to $I$}
		\FOR{$s=1$ to $S_\text{test}$}
		\STATE Obtain test draw, $x^{\{s\}}=X_\text{test}(s)\in \mathbb{R}^K$
		\STATE Predict chain id probabilities, $(p_1^{\{s\}},p_2^{\{s\}},...,p_N^{\{s\}})= \text{ML}(x^{\{s\}}|X_\text{train})$
		\STATE Draw a chain id, $c^{\{s\}} \sim \text{categorical}(p_1^{\{s\}},p_2^{\{s\}},...,p_N^{\{s\}})$
		\STATE Compare with actual id, $c^s$: $a^{\{s\}}=\mathbbm{1}(c^{\{s\}}=c^s)$
		\ENDFOR
		\STATE Calculate predictive accuracy, $\bar{a} = \frac{1}{S_\text{test}} \sum_{s=1}^{S_\text{test}} a^{\{s\}}$
		\STATE Calculate ratio to null model accuracy, $R^{*i} = \bar{a} / (1 / N) = N\bar{a}$
		\ENDFOR
		\RETURN $(R^{*1},R^{*2},...,R^{*I})$
	\end{algorithmic}
\end{algorithm}

\section{Results}\label{sec:results}
To illustrate the versatility of $R^*$, we use a range of examples that demonstrate how this statistic fares across a range of scenarios.

\subsection{Heterogeneity in chain variance: autoregressive example}\label{sec:heterogeneity}
We generate four Markov chains, where each samples from an autoregressive order 1 (AR(1)) process of the form,
%
\begin{equation}
X_t = \rho X_{t-1} + \epsilon_t
\end{equation}
%
where $\epsilon_t\stackrel{i.i.d.}{\sim}\mathcal{N}(0, \sigma)$, $\rho=0.3$ and $t=1,2,...,1000$. Three of the chains share the same $\sigma=1$, whereas the other chain has $\sigma=1/3$, so that it has $1/3$ of the (unconditional) variance of the others. In Fig. \ref{fig:ar1}A, we show how a GBM fitted to these data classifies observations according to a draw's value. Unsurprisingly, since the fourth chain has a smaller variance, observations close to zero are likely to be classified as being generated by this chain.

To illustrate the consistency of $R^*$, we perform 1000 replicates where, in each case, we generate four $\{X_t\}$ series as described (i.e. where one chain has a lower variance). We then fit a GBM to a labelled training set. The fitted model is then used to classify draws in an independent test set according to the chain which generated them. For each replicate, we then calculate $R^*$ as described in Algorithm \ref{alg:R_star}. In Fig. \ref{fig:ar1}B, we show that $R^*>1$ for 997/1000 replicates, indicating that the chains have not converged in a vast majority of cases. In Fig. \ref{fig:ar1}C, we show the rank-normalised split-$\widehat{R}$ as calculated in \cite{vehtari2019rank} for each replicate; as for $R^*$, this metric indicates the chains have not converged in all replicates.

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/ar1.pdf}}
	\caption{\textbf{Autoregressive example.} A shows how the GBM's classifications vary according to the draw's value for an example model fit; B shows 1000 $R^*$ draws as generated by Algorithm \ref{alg:R_star_uncertainty} across 1000 replicates; C shows corresponding rank-normalised split-$\widehat{R}$ values as calculated in \cite{vehtari2019rank} for each of the 1000 replicates; and D shows the $R^*$ distributions for the two series described in the text. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:ar1}
\end{figure}

Since GBMs return a probability simplex for each draw, which, for each chain, indicates the probability that the draw was generated by it, we can also generate a measure of uncertainty in $R^*$. Algorithm \ref{alg:R_star_uncertainty} describes in detail how this can be done. We demonstrate this idea using two datasets: one generated as described above, where one chain (out of four) has a lower variance than the others (we call this the ``unconverged'' data); and another, where all chains sample from the same distribution (we call this the ``converged'' data). In Fig. \ref{fig:ar1}D, we show the $R^*$ distributions in each case. For the unconverged data, the distribution has its bulk of mass away from 1, indicating lack of convergence. For the converged data, the distribution is centred on 1, indicating convergence. 

To investigate the sensitivity of $R^*$ to the GBM hyperparameters, we perform a sensitivity analysis. In this, we fit GBMs with six different sets of hyperparameters to replicates consisting of the same data as were used to generate Fig. \ref{fig:ar1}. In particular, the GBM hyperparameters we varied were the interaction depth (3, 5 or 7) and the number of trees (50, 100 or 200). The resultant $R^*$ point estimates across 200 replicates for each hyperparameter setting are shown in Fig. \ref{fig:ar1_sensitivity}. These show that hyperparameter choice affects $R^*$, with the highest median values obtained for an interaction depth of 3 with 50 trees (our default across all the examples we consider in this paper). This analysis does suggest, however, that this effect is relatively minor: indeed, across the various hyperparameter sets considered, the percentage of replicates where $R^*<1$ did not strongly vary.

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/ar1_sensitivity.pdf}}
	\caption{\textbf{Autoregressive example sensitivity to GBM hyperparameters.} The rows correspond to different GBM models fit using different hyperparameter sets (as indicated for interaction depth and number of trees and using a shrinkage of 0.1 and a minimum observations per node of 10 in all cases). The horizontal axis shows the $R^*$ values generated using Algorithm \ref{alg:R_star}; points have jitter added and are coloured according to whether they exceed the threshold $R^*=1$, with the percentage below this threshold annotated. The point-ranges shown indicate the 25\%, 50\% and 75\% quantiles for each model. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:ar1_sensitivity}
\end{figure}


\subsection{Diagnosing convergence in joint distributions: multivariate normal models}\label{sec:multivariate_normal}
To illustrate how $R^*$ can diagnose convergence issues in the joint target distribution, we consider a bivariate normal density. In all four chains, we use independent sampling to generate 1000 draws from bivariate normal densities with means of zero; in three of these chains, the covariance matrix is an identity matrix; in one chain, the covariance matrix also has unit diagonal terms but has off-diagonal terms of 0.9, indicating strong covariance between the two dimensions. By construction, all chains target the same marginal distribution in each dimension but the fourth chain has a different joint distribution.

First, we use the code provided in \cite{vehtari2019rank} to calculate rank-normalised $\widehat{R}$ and two different ESS measures, aiming to capture how well certain regions of the posterior have been explored: these are known as bulk ESS and tail ESS. In all cases, the various quantities were calculated based on chains split into halves. For both dimensions, the two ESS measures were above 3700, and $\widehat{R}<1.001$, indicating no issues with convergence.

Next, we estimate the $R^*$ distribution using Algorithm \ref{alg:R_star_uncertainty}, which is shown in Fig. \ref{fig:bivariate}. The mean of this distribution is 1.18, and all $R^*$ draws are above 1, indicating that the sampling distribution has not converged. By taking account of all the information in the chains, $R^*$ is able to probe issues in joint distribution convergence which are missed by measures that consider only marginals.

\begin{figure}[!htb]
	\centerline{\includegraphics[width=0.6\textwidth]{../output/bivariate.pdf}}
	\caption{\textbf{Bivariate normal example.} The distribution for $R^*$ across 1000 draws. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:bivariate}
\end{figure}

We next consider a more challenging problem -- a 250-dimensional multivariate normal target where its precision matrix, $\boldsymbol{A}\in\mathbb{R}^{250}\times\mathbb{R}^{250}$, is generated from a Wishart distribution \citep{hoffman2014no}. We assume that the Wishart distribution's degrees of freedom is 250, resulting in a distribution with high correlations between dimensions. We use Stan's NUTS algorithm to sample from this target distribution, and run the algorithm for two different iteration counts (each time across 4 chains): 400 and 10,000 (the latter thinned by a factor of 5). First, we used Stan to sample from the ``centered'' parameterisation of this model, which is of the form,
%
\begin{equation}\label{eq:mvt_normal_250}
\boldsymbol{x}\sim \mathcal{N}(\boldsymbol{0},\boldsymbol{A}^{-1}),
\end{equation}
%
where $\boldsymbol{x}\in\mathbb{R}^{250}$. For each set of draws, we used Algorithm \ref{alg:R_star_uncertainty} to generate an uncertainty distribution for $R^*$, which is shown in Fig. \ref{fig:mvt}A. From the plot for the 400 iteration case, it is clear that convergence has not yet occurred since $R^*>1$ across the bulk of this distribution. Even in the 10,000 iteration case, the $R^*$ distribution remains stubbornly shifted a little rightwards of $R^*=1$ (its mean is 1.03): in this case, $\widehat{R}<1.01$ for 95\% of parameters (Fig. \ref{fig:mvt}B), although 54\% had bulk $\text{ESS}<400$ and 13\% of parameters had tail $\text{ESS}<400$ indicating issues with convergence \citep{vehtari2019rank}.

Rather than run the MCMC sampler for more iterations, we move to a ``non-centered'' parameterisation, which introduces auxillary variables $\boldsymbol{z}\in\mathbb{R}^{250}$ that don't affect $p(\boldsymbol{x})$ but facilitate sampling from it. This model has the form,
%
\begin{align}
\boldsymbol{A}^{-1} = \boldsymbol{L}\boldsymbol{L}^T,\qquad
\boldsymbol{x} = \boldsymbol{L} \boldsymbol{z},\qquad
z_j\sim \mathcal{N}(0, 1), \text{ for } j = 1,2,...,250.
\end{align}
%
where $\boldsymbol{L}$ is the Cholesky decomposition of the covariance matrix, $\boldsymbol{A}^{-1}$. Fig. \ref{fig:mvt}A shows the $R^*$ distribution resultant from 10,000 NUTS iterations in this case: now the distribution is closer to $R^*=1$ (mean $R^*=1.01$). Fig. \ref{fig:mvt}B shows the $\widehat{R}$ values for each $x$ parameter in this model, and, echoing the result for $R^*$, $\widehat{R}<1.01$ in all cases; further, bulk- and tail-$\text{ESS}>400$ for all parameters.

In GBMs, it is possible to calculate variable importance (see, for example, \cite{friedman2001greedy} and \cite{greenwell2019package}), and this allows us to determine which variables were mostly responsible for predictive power. We now compare these with the more established metrics $\widehat{R}$ and ESS as calculated in Stan. For a GBM fitted to the centered model with 10,000 MCMC iterations (thinning by a factor of 5) for each chain, we plot in Fig. \ref{fig:mvt}C variable importance (here high values mean a variable is more important) versus $\widehat{R}$ for all dimensions of the target distribution (including Stan's $lp$ quantity, shown as a triangle). From this plot, there was a positive association between GBM's variable importance and $\widehat{R}$ (Spearman's rank correlation: $\rho=0.33, S=1763958, p<0.01$). In Fig.  \ref{fig:mvt}D, we plot variable importance versus two measures: bulk ESS and tail ESS, which both exhibited a strong non-linear negative association (Spearman's rank correlation: bulk ESS: $\rho=-0.57, S=4142470, p<0.01$; tail ESS: $\rho=-0.56, S=4113709, p<0.01$). Since none of these plots form perfect ``lines'' along which all the plotted points fall, this illustrates that variable importance provides information complementary to $\widehat{R}$ and ESS.

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1\textwidth]{../output/mvt_three.pdf}}
	\caption{\textbf{Multivariate normal example with 250 dimensions.} A shows $R^*$ distributions obtained for two MCMC samples (of differing numbers of draws: 400 and 1000) from the centered parameterisation (``cp'') and one from the non-centered version (``ncp''; with 1000 draws); B shows the rank-normalised split-$\widehat{R}$ values for all parameters from the same MCMC runs as in A; C shows variable importance versus $\widehat{R}$ for each parameter; and D shows variable importance versus bulk- and tail-ESS as calculated by \cite{vehtari2019rank}. In A, 1000 $R^*$ draws by Algorithm \ref{alg:R_star_uncertainty} are shown for each MCMC run. In plots B and C, horizontal jitter was added to the points and a loess fit line with standard errors overlaid. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:mvt}
\end{figure}

\subsection{Wide datasets: multivariate normal}\label{sec:wide}
As the number of parameter dimensions increases, it might be thought that ML algorithms will overfit the data, and, hence, testing set classification would be poor; leading to unreliable determinations of convergence. To test this hypothesis, we investigated two scenarios using a multivariate normal target. In the first of these, we used the 250-dimensional multivariate normal of eq. \eqref{eq:mvt_normal_250} with 250 post-warm-up iterations (after 250 warm-up iterations) for each of 4 chains from Stan to calculate $R^*$ distributions as in Algorithm \ref{alg:R_star_uncertainty}. Here, we considered both the centered and non-centered parameterisations, where, in both cases, the number of iterations is comparable to the number of parameters, so the training data is relatively ``wide'': the $R^*$ distribution in each case is shown in Fig. \ref{fig:mvt_wide_both}A. In this figure, it is clear that both $R^*$ distributions are shifted away from 1, indicating non-convergence. The same conclusion is reached if rank-normalised split-$\widehat{R}$ is used instead (Fig. \ref{fig:wide_both_diagnostics}A), since, for both parameterisations, some of the parameters had $\widehat{R}>1.01$. Using bulk- or tail-ESS instead, we conclude that the non-centered parameterisation shows signs of convergence whereas the centered does not (Fig. \ref{fig:wide_both_diagnostics}B\&C). 

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1\textwidth]{../output/mvt_wide_both.pdf}}
	\caption{\textbf{Wide data examples.} A shows the $R^*$ distribution for the 250-dimensional example in \S\ref{sec:wide} with 250 post-warm-up iterations per chain from Stan's NUTS algorithm across both model parameterisations; B shows $R^*$ distribution for the 10,000-dimensional example with 400 and 1000 MCMC iterations per chain (although the first half of these were discarded as warm-up). In all cases, 1000 draws of $R^*$ are plotted as generated by Alg. \ref{alg:R_star_uncertainty} for a single MCMC run composed of 4 chains. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:mvt_wide_both}
\end{figure}

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1\textwidth]{../output/wide_both_diagnostics.pdf}}
	\caption{\textbf{Wide data 250-dimensional example: established diagnostics.} The top row shows the results for the centered parameterisation; the bottom row for the non-centered. Column A shows split-$\widehat{R}$; columns B and C show the bulk- and tail-ESS; in each case the statistics are displayed for all model parameters and were calculated using 250 post-warm-up draws from Stan's NUTS algorithm. In both cases, the results correspond to a single MCMC run composed of 4 chains. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:wide_both_diagnostics}
\end{figure}

We next consider a more challenging example -- a target distribution with 10,000 dimensions. In this case, we assume independent standard normals for each dimension. In Fig. \ref{fig:mvt_wide_both}B, we plot the $R^*$ distribution for two MCMC runs targeting this distribution: one with 400 iterations, the other with 1000. In both cases, the distributions were right of $R^*=1$, indicating non-convergence. These results were also echoed by rank-normalised split-$\widehat{R}$, with 65\% of dimensions having $\widehat{R}>1.01$ for the 400 iteration case and 19\% for the 1000 iteration case.

Overall, the examples in this section suggest that $R^*$ is robust to wide datasets; we also note that the statistic took comparable time to calculate relative to existing convergence diagnostics on a desktop computer.

\subsection{Infinite variance: Cauchy example}
We next explore how $R^*$ can be used to determine convergence for distributions with infinite variance. Like \cite{vehtari2019rank}, we first use Stan to sample from independent standard Cauchy distributions for each element of a 50-dimensional vector $x$,
%
\begin{equation}
x_j\sim \text{Cauchy}(0, 1),\; \text{for } j=1,...,50.
\end{equation}
%
We call this parameterisation the ``nominal'' version of this model.

In addition, we also use Stan to sample from an ``alternative'' parameterisation of the Cauchy, based on a scale mixture of Gaussians \citep{vehtari2019rank},
%
\begin{align}
a_j \sim  \N(0,1), \qquad
b_j \sim  \text{Gamma}(0.5, 0.5), \qquad
x_j =  a_j/\sqrt{b_j}.
\end{align}
%
The distribution of the $x$ vector is the same under both parameterisations, although the thin-tailed $(a,b)$ vectors define a higher dimensional posterior that improves sampling efficiency.

In the top-left panel of Fig. \ref{fig:cauchy}, we show the $R^*$ distribution under both parameterisations. As shown in \cite{vehtari2019rank}, the nominal parameterisation results in poor sampling efficiency due to its long tails, meaning that, after 1000 MCMC post-warm-up iterations (with 1000 warm-up iterations discarded) across each of four chains, draws still contain information about chain identity, and, accordingly, the $R^*$ distribution is shifted rightwards from $R^*=1$. The alternative parameterisation fares better and the $R^*$ distribution is nearer $R^*=1$, yet, still the mean is above this value. In the bottom-left panel of Fig. \ref{fig:cauchy}, we show the rank-normalised split-$\widehat{R}$ values across each of the 50 parameters for the same MCMC runs. The nominal parameterisation has some parameters with $\widehat{R}>1.01$, indicative non-convergence, whereas the alternative has $\widehat{R}>1.01$ for all parameters.

Since the $R^*$ distribution indicated non-convergence for both parameterisations, we ran each model for sixty-times as long, although thinned by a factor of 3, resulting in 10,000 post-warm-up iterations across each of four chains. In the right-hand column of Fig. \ref{fig:cauchy}, we show the results for these longer runs. In these, the alternative parameterisation now has an $R^*$ distribution centered on $R^*=1$ and, hence, we are more confident that convergence has occurred. Despite the added iterations, the $R^*$ distribution from the nominal model remains stubbornly away from 1. The $\widehat{R}$ values are all below 1.01, indicating convergence in both cases.

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/cauchy.pdf}}
	\caption{\textbf{Cauchy example.} Columns show MCMC runs with 1000 (left) and 10,000 (right; obtained by thinning iterations by a factor of 3) post-warm-up iterations (each with half iterations discarded as warm-up) for each of four chains. Rows show the $R^*$ distributions (top) and rank-normalised split-$\widehat{R}$ values across all parameters (bottom). The different shadings indicate different model paramerisations as indicated in legend. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:cauchy}
\end{figure}

To illustrate that $R^*$ provides a reliable metric for capturing convergence, we now calculate a quantitative measure that captures how closely a sampling distribution matches the target. One measure of distributional ``closeness'' is the KL-divergence, which, in this case, could be used to measure the divergence from target to sampling distribution: if the target distribution is known, fitting a kernel density estimator (KDE) to samples allows an approximate (typically univariate) measure of KL-divergence to be calculated for each dimension. The trouble is, for distributions like the Cauchy with fat tails, fitting a KDE to the samples provides a noisy measure of the sampling distribution in the tails. This means that approximate KL-divergence is unreliable for these types of model. Instead, we choose a measure of distributional discrepancy based around similarity between target quantiles and sample-estimated equivalents. Specifically, we calculate the $R^2$ for the regression of actual quantile values on sample-estimated quantiles, where, if $R^2\sim 1$, the sampling distribution recapitulates well the target quantities. In our example, we consider all percentiles: 0.1\%, 0.2\%,...,99.8\%, 99.9\% and calculate the mean $R^2$ across all 50 dimensions.

In Fig. \ref{fig:cauchy_convergence}A, we plot this ``quantile $R^2$'' as a function of MCMC sample size for both parameterisations of the Cauchy model. This shows that after c.10,000 iterations, the alternative parameterisation approaches $R^2\approx 1$; at the same number of iterations, the nominal parameterisation still provides a poor measure of tail quantiles. Next, in Fig. \ref{fig:cauchy_convergence}B and Fig. \ref{fig:cauchy_convergence}C, we plot two measures of $\widehat{R}$, each calculated from splitting the 4 original chains into two equal halves. The first of these measures is the rank-normalised $\widehat{R}$ \citep{vehtari2019rank}, which provides a separate measurement for each target dimension; in Fig. \ref{fig:cauchy_convergence}B, we show how the maximum of this measurement across all 50 dimensions changes with sample size. After c.550 iterations, the alternative parameterisation achieves $\widehat{R}<1.01$ for all target dimensions, and, after c.10,400 iterations, the nominal model achieves the same maximum $\widehat{R}$ value: in both cases, these suggest convergence. The second measure is multivariate $\widehat{R}$ \citep{brooks1998general}, which, like $R^*$, yields a single measurement across all dimensions; Fig. \ref{fig:cauchy_convergence}C shows how this metric changes with sample size for both Cauchy model parameterisations. After c. 1800 iterations, multivariate $\widehat{R}<1.01$ for the alternative parameterisation, whilst after 25,000 iterations multivariate $\widehat{R}>1.07$ indicating more draws are needed. In Fig. \ref{fig:cauchy_convergence}D, we plot $R^*$ against iteration for both models: these indicate that, after 25,000 iterations, $R^*\approx 1.05$ for the nominal model, and $R^*>2$ for the nominal parameterisation: both these $R^*$ values suggest lack of convergence. Finally, in Figs. \ref{fig:cauchy_convergence}E\&F, we plot the minimum across all the dimensions of tail- and bulk-ESS calculated as described in \cite{vehtari2019rank}. After c.180 iterations, the alternative parameterisation surpassed a tail-ESS of 400; after c.18,700, the nominal parameterisation did the same. Both models were quicker to pass 400 bulk-ESSs.

Comparing our ``omniscient'' measure of convergence shown in Fig. \ref{fig:cauchy_convergence}A, with the various derived measures, all show a similar pattern: as sample size increases, the various statistics tend towards convergence. The rate at which these converge differs though, and $R^*$ (Fig. \ref{fig:cauchy_convergence}D) appears at least, qualitatively, most similar to our ``omniscient'' measure.

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/cauchy_convergence.pdf}}
	\caption{\textbf{Measuring convergence for the Cauchy model.} A shows a measure of convergence, the mean quantile $R^2$, that requires knowing the target distribution; B shows the maximum value of split-$\widehat{R}$ across each of the 50 dimensions of the target; C shows the multivariate split-$\widehat{R}$ value; D shows the value of split-$R^*$ as calculated by Algorithm \ref{alg:R_star}; and E and F show tail- and bulk-ESS. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:cauchy_convergence}
\end{figure}

\subsection{Non-stationary marginals}
To determine whether $R^*$ can detect non-stationary sampling distributions, we recapitulate an example from appendix A in \cite{vehtari2019rank}. This example showed that split-$\widehat{R}$ could detect non-convergence caused by shifts in sampling distributions over time: in their case, they analysed chains with common linear trends in mean. Specifically, they first generated four chains by random sampling from a univariate normal distribution, then added a common time trend to each chain, resulting in a univariate distribution whose mean increased during sampling. We first repeat this analysis but using $R^*$ rather than $\widehat{R}$: in Fig. \ref{fig:trends_all_dim}, we show these results. In column A, we show the results for $R^*$ calculated on the four chains that ran; column B shows the same calculation but after the chains are split into two equal halves. The rows show the range of sample sizes investigated: 250, 1000 and 4000; the horizontal axis shows the magnitude of time trend added to each sample; for all parameter sets, we run 10 replicates. This plot mirrors Fig. 18 in \cite{vehtari2019rank} and shows that, without splitting the chains, $R^*$ does not increase with trend whereas, after splitting, it does. As expected, split-$R^*$ is more reliably able to detect non-convergence as sample size increases.

These results make intuitive sense: without systematic between-chain variation, it is not possible to reliably determine which of them caused a particular observation. In this case, because all chains exhibited the same secular trends over time, there would not be differences in their marginals. By splitting chains into two -- the first half being the early phase, and the second half being the later phase with higher mean -- this forces differences in the marginals. This meant it was possible to reliably pick whether an observation was caused by an early phase chain or a later one. As such, we recommend that $R^*$ always be calculated using split chains as is recommended for $\widehat{R}$ \citep{carpenter2017stan,vehtari2019rank}.

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/trends_all_dim.pdf}}
	\caption{\textbf{Univariate trends example.} Column A represents results for $R^*$ calculated using Algorithm \ref{alg:R_star} on the four chains; column B  shows the same calculation after each chain is split into two halves. The rows present the differing sample sizes. The horizontal axis measures (half) the change in mean across the whole sample: so a value ``1'' indicates the mean increases by 2 units from the start to end of sampling. At each parameter set, 10 replicates were run and jitter was added to the points. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:trends_all_dim}
\end{figure}

We next consider whether split-$R^*$ can detect non-convergence when only a single dimension trends. In Fig. \ref{fig:trends_one_dim}, we show how $R^*$ performs across a range of target dimensions. In the simulations here, all dimensions bar one are stationary; the remaining dimension has a linear trend added to it. In all cases, split-$R^*$ increased with trend. Indeed, differences in the typical values of this metric were not apparent across the different target dimensions considered. This suggests split-$R^*$ can robustly determine chain identity if only a single dimension has a non-stationary sampling distribution.


\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/trends_one_dim.pdf}}
	\caption{\textbf{Multivariate trends example with split-$R^*$.} The columns present different dimensionalities of the target distribution; the rows present different sample sizes. The horizontal axis measures (half) the change in mean across the single dimension that had a trend added to it: a value ``1'' indicates its mean increases by 2 units from the start to end of sampling; all other dimensions (if dimensions exceeded 1) had stationary distributions. At each parameter set, 10 replicates were run and jitter was added to the points. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:trends_one_dim}
\end{figure}

Means that trend over time is one form of non-stationarity; another is a time-varying covariance. Next, we consider a bivariate normal with (constant) standard normal marginals, but where the correlation between dimensions trends over time. Specifically, we allow the correlation to increase linearly from $-\rho$ and $\rho$ throughout the course of simulations, and use i.i.d. draws from the process across 4 ``chains''. Again, as before, $R^*$ calculated on unsplit chains is unable to detect this form of non-stationarity, since there are no inter-chain differences in the sampling distribution. Similarly, split-$\widehat{R}$ does not detect this form of non-convergence since the marginal distribution across chains does not vary over time (Fig. \ref{fig:trends_joint_distribution}A). By contrast, split-$R^*$ can (Fig. \ref{fig:trends_joint_distribution}B), since it uses all information in the samples, including the covariance structure.

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/trends_joint_distribution.pdf}}
	\caption{\textbf{Bivariate normal with trending correlation example.} Column A shows the results for split-$\widehat{R}$; column B for $R^*$; the rows present the differing sample sizes. The horizontal axis measures (half) the change in correlation across the whole sample: so a value ``0.5'' indicates the correlation increases by 1 unit (from -0.5 to 0.5) from the start to end of sampling. At each parameter set, 10 replicates were run and jitter was added to the points. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:trends_joint_distribution}
\end{figure}


\subsection{Hierarchical model: Eight schools model}\label{sec:eight_shools}
We now examine a simple classic example used to highlight difficulties in performing inference for hierarchical models: referred to as the ``Eight school'' model (see Section 5.5 in \citep{gelman2013bayesian}), which aimed to determine the effects of coaching on SAT scores in eight schools. 

The model can be parameterised two ways, as described in \cite{vehtari2019rank}. The simplest way is referred to as the ``centered'' parameterisation and exactly mirrors the underlying statistical model,
%
\begin{align*}
\theta_j &\sim \N(\mu, \tau), \\
y_j &\sim \N(\theta_j, \sigma_j).
\end{align*}
%
The ``non-centered'' parameterisation recodes this model in a way which does not affect the joint distribution of $(\theta, \mu, \tau, \sigma)$ but makes it easier to sample from it, by introducing auxillary variables, $\tilde \theta_j$. This can be written as,
%
\begin{align*}
\tilde{\theta}_j &\sim \N(0, 1), \\
\theta_j &= \mu + \tau \tilde{\theta}_j,\\
y_j &\sim \N(\theta_j, \sigma_j).
\end{align*}
%
In both cases, $\theta_j$ are the treatment effects in the eight schools, and $(\mu, \tau)$ represent the population mean and standard deviation 
of the distribution of these effects. In the centered parameterization, the $\theta_j$ are parameters, whereas in the non-centered parameterization, the $\tilde{\theta}_j$ are parameters and $\theta_j$ is a derived quantity.

We first used Stan \citep{carpenter2017stan} to sample from the centered model using 4 chains. Like \cite{vehtari2019rank}, we used settings that reduce the chance of divergent iterations for the NUTS algorithm \citep{hoffman2014no}, meaning that the resultant sampling distribution is likely to be biased. We also used Stan with the same algorithm settings to sample from the non-centered model.

To see how $R^*$ performed on this example, we first split each of the (post-warm-up) chains in two, as is done by default in Stan \citep{carpenter2017stan} and in \cite{vehtari2019rank}, resulting in 500 iterations across 8 chains. Following the same approach as in Algorithm \ref{alg:R_star_uncertainty}, we generated $R^*$ distributions for both the centered and non-centered models. The resultant distributions for $R^*$ are shown in Fig.\ref{fig:eight_schools}A. In this plot, the centered model shows signs of convergence, whereas the non-centered model does not.

In addition, to illustrate the power of $R^*$, we also repeat the analysis but, this time, do not split the chains in two. The results are shown in Fig.\ref{fig:eight_schools}B. In this case, because the unsplit chains do not mix with themselves, it is harder to accurately predict the chain that generated each draw, meaning that the centered model $R^*$ values are shifted leftwards. Despite this, however, the centered model distribution for $R^*$ still does not strongly overlap with $R^*=1$, indicating that the model has not converged, contrasting with the non-centered model which appears converged.

$\widehat{R}$, like $R^*$, is recommended to be calculated using split chains. In Fig. \ref{fig:eight_schools}C, we plot the values of values of this metric when using the original four chains (horizontal axis) versus those when using the split chains (vertical axis) for the ten parameters in this model; we do this for both the centered and non-centered models. These show that the values of $\widehat{R}$ for the non-centered model for both the split and unsplit cases were, for all parameters, above 1.01, echoing the results for $R^*$; also, like $R^*$, the value of $\widehat{R}$ is higher for the split case. Finally, as for $R^*$, all parameters for the centered models were below 1.01, indicating convergence.

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/eight_schools.pdf}}
	\caption{\textbf{Eight schools example: $R^*$ distributions.} A shows draws from the $R^*$ distribution when splitting chains in two (resulting in 8 chains); B shows the same but using the 4 original chains; C shows rank-normalised $\widehat{R}$ for original 4 chains versus those for the 8 chains case for all ten parameters defined by the centered model -- in this case, we plot horizontal and vertical dashed lines to illustrate the $\widehat{R}=1.01$ cutoff and a $y=x$ line. In all cases, the plots show 1000 $R^*$ draws. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:eight_schools}
\end{figure}

To understand the predictive power of the ML better, we investigated how predictive accuracy varies across parameter space. After fitting the GBM model, we group MCMC draws into deciles and draw from the $R^*$ distribution for each decile. In Fig. \ref{fig:eight_schools_r_star_quantiles}, we show the results of this exercise for (A) $\mu$ and (B) $\tau$. In the top row of this figure, we show the path of four MCMC chains (here we did not split chains when calculating $R^*$ to simplify visualisations) across the quantiles of each parameter space. Above the top panel, we show the marginal distributions for each chain. In the bottom row, we show 40 $R^*$ draws for each decile, which were generated according to Algorithm \ref{alg:R_star_uncertainty} using a GBM fit to all the draws. In essence, the top rows explain the variation in $R^*$ in the bottom panels: if chains become stuck in regions of parameter space, this causes differences between the marginal distributions of the chains; these differences, in turn, allow a ML model to predict the generative chain in those same sticky regions. For example, for $\mu$, the purple chain became stuck around the middle quantile, forcing a difference in its marginal distribution in that region, which resulted in $R^*>1$ for the corresponding decile. Similarly, for $\tau$, the purple chain became stuck in the lowest quantiles, elevating its marginal distribution there and resulting in improved ML predictive accuracy.

Fig. \ref{fig:eight_schools_r_star_quantiles} also indicates a potential limitation of $R^*$: namely, that as chains are progressively thinned, those regions where chains behave most idiosyncratically can be missed, resulting in a reduction in ML classification accuracy and falsely concluding that convergence has occurred.

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/eight_schools_r_star_quantiles.pdf}}
	\caption{\textbf{Eight schools example: quantile $R^*$ plots.} Column A shows plots for $\mu$; Column B for $\tau$. In each column, we show the path of the four individual chains above and 40 $R^*$ draws for each parameter quantile below. Above the top row, we show the marginal distribution of each chain estimated via kernel density estimation using Gaussian kernels. Note that, in the bottom plots, jitter has been added to the data points. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:eight_schools_r_star_quantiles}
\end{figure}

\subsection{Many parameter models: ovarian and prostate analysis}
In this section, we analyse two Bayesian models [Aki, can you provide some description of these?]. These models have relatively many parameters (4719 for the ``ovarian'' model and 18,105 for the ``prostate'' model) and are known to be challenging to explore with MCMC. For each model, we consider two MCMC runs: one with 4 chains run with 10,000 iterations each (1000 discarded as warm-up and thinned by a factor of 10); another with 16 chains run with 1500 iterations each (500 discarded as warm-up and no thinning).

For the ovarian model, we show the results in Fig. \ref{fig:ovarian}. In Fig. \ref{fig:ovarian}A, we show the $R^*$ distributions for each model run, which show that, whereas the ``long'' model run has converged, the ``short'' one has yet to do so. Whilst it is harder to discern, this pattern is mirrored in Fig. \ref{fig:ovarian}B, since the long model has $\widehat{R}<1.01$ for all parameters [check this!], whereas the short model has a handful of parameters where this is not the case. Similarly, in Figs. \ref{fig:ovarian}C and \ref{fig:ovarian}D, which show the bulk ESS and tail ESS respectively, it is evident that, for the short model, there remain a few parameters with low effective sample sizes, whereas the long model has more consistent values for this metric.

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/ovarian.pdf}}
	\caption{\textbf{Ovarian example.} In all plots, we show the results from two model runs: one with 4 chains with 10,000 iterations each (1000 discarded as warm-up and thinned by a factor of 10); another with 16 chains run with 1500 iterations each (500 discarded as warm-up and no thinning). In A, $R^*$ distributions (with 1000 draws each using Algorithm \ref{alg:R_star_uncertainty}) are shown; B shows rank-normalised split-$\widehat{R}$ values across all parameters; C shows bulk ESS across all parameters; and D shows tail ESS across parameters. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:ovarian}
\end{figure}

For the prostate model, we show the results in Fig. \ref{fig:prostate}. Since this model has nearly four times as many parameters as the ovarian model, it was more computationally expensive to estimate $R^*$ for it. To handle this, we thinned down the parameters by a factor of 5 for the long model, recognising that, of course, this measure will make it more likely that we diagnose convergence. Despite this, both $R^*$ measures indicated that the MCMC runs had yet to converge (Fig. \ref{fig:prostate}A), which was mirrored by the other metrics considered (Figs. \ref{fig:prostate}B,C\&D).

\begin{figure}[!htb]
	\centerline{\includegraphics[width=1.0\textwidth]{../output/prostate.pdf}}
	\caption{\textbf{Prostate example.} In all plots, we show the results from two model runs: one with 4 chains with 10,000 iterations each (1000 discarded as warm-up and thinned by a factor of 10); another with 16 chains run with 1500 iterations each (500 discarded as warm-up and no thinning). In A, $R^*$ distributions (with 1000 draws each using Algorithm \ref{alg:R_star_uncertainty}) are shown -- for the long run, these were calculated after thinning the parameters by a factor of 5; B shows rank-normalised split-$\widehat{R}$ values across all parameters; C shows bulk ESS across all parameters; and D shows tail ESS across parameters. See the R markdown file on the Github repository for code to reproduce this example.}
	\label{fig:prostate}
\end{figure}

\section{Discussion}
If an MCMC sampler has converged on the target distribution, the chains must be well-``mixed'', that is, given a draw, it should be impossible to discern which chain generated it. Based on this observation, we used supervised machine learning (ML) classifiers to quantify the information about the generative chain identity contained in draws. By taking the ratio of ML model predictive accuracy obtained on an independent test set to the accuracy of a null model (which predicts a chain's identity uniformly at random), this defines our $R^*$ statistic. By extracting ML-predicted chain probabilities from each prediction in the test set, we can additionally generate an uncertainty distribution for $R^*$. Across a range of previously published examples, $R^*$ was shown to be predictive of whether chains had converged.

The predominant methods for diagnosing MCMC convergence rely heavily on looking for between-chain differences in the marginal distributions along each dimension of the target. $R^*$ naturally includes this information in building a model capable of predicting the chain that generated each draw. It also naturally includes information about the joint distribution across all dimensions of the target. Since converged chains should have similar joint distributions (implying similar marginals), any measure of convergence should account for both of these aspects. Indeed, in \S\ref{sec:multivariate_normal}, we show that more established measures may indicate converge whereas $R^*$ shows otherwise. This is not a sleight on existing measures, more that this illustrates the complementarity of $R^*$ to them.

When first starting to develop $R^*$, it was unclear to us whether \textit{any} metric aligned with a supervised ML method would be overly sensitive to its hyperparameters. With gradient-boosted regression tree models, we found this not to be the case. Indeed, after a little experimenting, we found that a single set of hyperparameters (which we report in \S\ref{sec:method}) sufficed across all our examples. To ensure maximal predictive capability of any fitted ML model, however, it is prudent to optimise it over choice of hyperparameters, and we recommend that applied users consider including this step in their workflow.

Many implementations of $\widehat{R}$ suggest splitting chains in two before calculating it. In a number of examples, we trial this before calculating $R^*$, and find that this approach leads to more accurate chain prediction. As such, we recommend that this practice be adopted whenever $R^*$ is calculated to ensure that this measure is maximised. Additionally, our ML calculation method for $R^*$ makes it possible to include any covariates which may be useful features for prediction, such as an ``iteration block'' indicator variable taking values $1, 2, ..., K$ in each of $K$ blocks of contiguous iterations. If each chain is thoroughly mixed with itself, including this additional information shouldn't change $R^*$; by contrast, if the chains are random walk-like, this information should boost $R^*$.

MCMC enables inference across a wide range of models encountered across the social, biological and physical sciences. Its ease of implementation, however, masks important underlying fragilities in the method. Namely, that unless the chains have converged to a truly stationary distribution, the draws generated are not faithful depictions of the posterior. In this paper, we introduce a new metric, $R^*$, that is especially good at diagnosing poor convergence in the joint sampling distribution -- an area that we argue has received insufficient attention thus far. $R^*$ can straightforwardly be introduced into existing MCMC libraries and could provide a measure of convergence complementary to existing metrics.

	
\bibliographystyle{chicago}
\bibliography{bibliography} 
	
\end{document}
