---
title: "Cauchy example"
output: html_notebook
---

```{r}
rm(list=ls())
library(tidyverse)
library(reshape2)
library(rstan)
library(latex2exp)
library(mvtnorm)
library(goftest)
options(mc.cores=4)
rstan_options(auto_write = TRUE)
source("monitornew.R")
source("r_star_monitor.R")
```


## Few iterations
Generate split-R* distribution from nominal Cauchy fit
```{r}
fit_nom <- stan(file = 'cauchy_nom.stan', seed = 7878,
                refresh = 0)
x <- rstan::extract(fit_nom, permuted=F)

r_star_nom <- r_star_dist(x)
```

Generate split-R* distribution from alternative Cauchy parameterisation
```{r}
fit_alt1 <- stan(file = 'cauchy_alt_1.stan',
                 seed = 7878, refresh = 0)
full_data <- rstan::extract(fit_alt1, permuted=F)

r_star_alt <- r_star_dist(full_data)
```

Plot split-R* values for each parameterisation
```{r}
a_df <- tibble(nominal=r_star_nom,
               alt=r_star_alt)
g <-
  a_df %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=as.factor(variable))) +
  geom_histogram(position="identity", alpha=0.8) +
  scale_fill_grey("Parameterisation", labels=c("nominal", "alternative")) +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  theme(text = element_text(size=14, colour="black"),
        legend.position = "none")
g
```

## Calculate KL divergence from true density by fitting kernel density estimator to points then integrating
nominal parameterisation
```{r}
x <- rstan::extract(fit_nom, permuted=F)

sample <- as.vector(x[, , i])
dens <- density(sample)
f_approx <- approxfun(dens$x, dens$y, rule=2)
integrate(
    function(x) f_approx(x) * log(f_approx(x) / dcauchy(x)),      -20, 20, subdivisions = 1000)

t <- seq(-20, 20, 0.01)
tibble(t,
  true=map_dbl(t, ~dcauchy(.)),
  approx=map_dbl(t, ~f_approx(.))) %>%
  melt(id.vars="t") %>%
  ggplot(aes(x=t, y=value, colour=as.factor(variable))) +
  geom_line()

f_kl_approx <- function(sample, limit=20){
  dens <- density(sample)
  f_approx <- approxfun(dens$x, dens$y, rule=2)
  kl <- integrate(function(x) f_approx(x) * log(f_approx(x) / dcauchy(x)),      -limit, limit, subdivisions=1000)$value
  if(kl < 0)
    return(0)
  else
    return(kl)
}

kl <- vector(length = 50)
for(i in seq_along(kl))
  kl[i] <- f_kl_approx(x[, , i], limit=20)
```

Density estimation fails for alternative parameterisation because of fat tails
```{r}
names(extract(fit_alt1))
x <- rstan::extract(fit_alt1, permuted=F, pars="x")
sample <- as.vector(x[, , 3])
dens <- density(sample, 0.001)
f_approx <- approxfun(dens$x, dens$y, rule=2)
integrate(function(x) f_approx(x), -10, 10)
integrate(
    function(x) f_approx(x) * log(f_approx(x) / dcauchy(x)),      -100, 100, subdivisions = 1000)

t <- seq(-20, 20, 0.01)
tibble(t,
  true=map_dbl(t, ~dcauchy(.)),
  approx=map_dbl(t, ~f_approx(.))) %>%
  melt(id.vars="t") %>%
  ggplot(aes(x=t, y=value, colour=as.factor(variable))) +
  geom_line()

f_kl_approx <- function(sample, limit=20){
  dens <- density(sample)
  f_approx <- approxfun(dens$x, dens$y, rule=2)
  kl <- integrate(function(x) f_approx(x) * log(f_approx(x) / dcauchy(x)),      -limit, limit, subdivisions=1000)$value
  if(kl < 0)
    return(0)
  else
    return(kl)
}

kl <- vector(length = 50)
for(i in seq_along(kl))
  kl[i] <- f_kl_approx(x[, , i], limit=20)
```

Look at quantiles instead using KS test statistic: problem with KS test statistic is that it is most sensitive to middle values: https://asaip.psu.edu/articles/beware-the-kolmogorov-smirnov-test/
```{r}
x <- rstan::extract(fit_alt1, permuted=F, pars="x")
sample <- as.vector(x[, , 1])
ks.test(sample, pcauchy)
ks <- vector(length = 50)
for(i in 1:50)
  ks[i] <- ks.test(x[, , i], pcauchy)[1]$statistic
```

Anderson-darling test -- by comparing with an independent sampler we get a gauge of the quality of MCMC
```{r}
x <- rstan::extract(fit, permuted=F, pars="x")

f_calculate_ad <- function(x){
  dims <- dim(x)
  nparams <- dims[3]
  ad <- vector(length = nparams)
  for(i in 1:nparams)
    ad[i] <- ad.test(x[, , i], pcauchy)[2]$p.value
  return(ad)
}

# keep sample size fixed by thinning
nmax <- 5000
iter <- seq(500, nmax, 50)
ad <- vector(length = length(iter))
for(i in seq_along(iter)){
  a_iter <- iter[i]
  ele <- round(seq(1, a_iter, length.out = 500))
  ad[i] <- mean(f_calculate_ad(x[ele, ,]))
}

e_df <- tibble(iter, ad, source="MCMC")

# compare with independent samples
# ps <- map_dbl(seq(1, 1000, 1), function(i) ad.test(rcauchy(1000), pcauchy)[2]$p.value)
# hist(ps)

x <- array(rcauchy(2 * nmax * 50), c(nmax, 4, 50))

iter <- seq(500, nmax, 50)
ad <- vector(length = length(iter))
for(i in seq_along(iter)){
  a_iter <- iter[i]
  ele <- round(seq(1, a_iter, length.out = 500))
  ad[i] <- mean(f_calculate_ad(x[ele, ,]))
}
f_df <- e_df %>% 
  bind_rows(tibble(iter, ad, source="independent"))
f_df %>% 
  ggplot(aes(x=iter, y=ad, colour=as.factor(source))) +
  geom_line()
```

Try R* with independent sampler
```{r}
nmax <- 100
x <- array(rcauchy(2 * nmax * 50), c(nmax, 4, 50))
r_star_inde <- r_star_dist(x)
hist(r_star_inde)
```

Possible to tell whether sample came from MCMC or independent?
```{r}
x_mcmc <- rstan::extract(fit, permuted=F, pars="x")
x_mcmc <- array(rcauchy(2000 * 50), c(2000, 50)) %>% 
  as.data.frame() %>% 
  mutate(source="mcmc")

f_distribution <- function(x_mcmc){
  x_inde <- array(rcauchy(2000 * 50), c(2000, 50))
  x_inde <- x_inde %>% 
  as.data.frame() %>% 
  mutate(source="independent")
  r <- bind_rows(x_mcmc,
                 x_inde) %>% 
    mutate(source=as.factor(source))
  rand_samples <- sample(1:nrow(r), 0.7 * nrow(r))
  training_data <- r[rand_samples, ]
  testing_data <- r[-rand_samples, ]
  
  gbmFit1 <- train(source ~ ., data = training_data, 
                     method = "gbm",
                     trControl = trainControl(method = 'none'), 
                     tuneGrid = caretGrid, verbose=FALSE)
  
  plda <- predict(object=gbmFit1, newdata=testing_data)
  a_accuracy <- 
    tibble(predicted=plda, actual=testing_data$source) %>%
    mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
    summarise(mean(correct)) %>% 
    pull()
  return(a_accuracy)
}
library(furrr)
future::plan(multiprocess)
laccuracy <- future_map_dbl(seq(1, 1000, 1), function(i) f_distribution(x_mcmc))
hist(laccuracy, 50)
```

Could do same test with simply empirical distributions, also, by bootstrapping


## 10,000 iterations
Nominal model with 10000 iterations and thinning by 10
```{r}
fit <- stan(file = 'cauchy_nom.stan', seed = 7878,
            refresh = 0, iter = 50000)
saveRDS(fit, "../output/fit_nominal_50000.rds")
full_data <- rstan::extract(fit, permuted=F)

r_star_nom <- r_star_dist(full_data)
```

Alternative model with 10000 iterations and thinning by 10
```{r}
fit <- stan(file = 'cauchy_alt_1.stan', seed = 7878,
            refresh = 0, iter = 50000)
saveRDS(fit, "../output/fit_alt_50000.rds")
full_data <- rstan::extract(fit, permuted=F)

r_star_alt <- r_star_dist(full_data)
```

Plot
```{r}
b_df <- tibble(nominal=r_star_nom,
               alt=r_star_alt)
 
g1 <-
  a_df %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=as.factor(variable))) +
  geom_histogram(position="identity", alpha=0.8) +
  scale_fill_grey("Parameterisation", labels=c("nominal", "alternative")) +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  theme(text = element_text(size=14, colour="black"),
        legend.position = "none") +
  ggtitle("A.")
g1

g2 <-
  b_df %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=as.factor(variable))) +
  geom_histogram(position="identity", alpha=0.8) +
  scale_fill_grey("Parameterisation", labels=c("nominal", "alternative")) +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  theme(text = element_text(size=14, colour="black")) +
  ggtitle("B.")
g2

pdf("../output/cauchy.pdf", width = 12, height = 6)
multiplot(g1, g2, cols = 2)
dev.off()
```

Compare quantiles
```{r}
fit <- readRDS("../output/fit_nominal_50000.rds")
x <- rstan::extract(fit, permuted=F, pars="x")
r_squared <- function(x, lower=0.001){
  qs <- seq(lower, 1-lower, lower)
  q_actual <- map_dbl(qs, ~qcauchy(.))
  r_vals <- vector(length = 50)
  for(i in seq_along(r_vals)){
    q_approx <- quantile(x[, , i], qs)
    aLM <- lm(data=tibble(q_approx, q_actual), formula = q_actual~q_approx)
    a_sum <- summary(aLM)
    r_vals[i] <- a_sum$r.squared
  }
  return(r_vals)
}

log_last <- log10(25000)
log_first <- log10(100)
iter <- round(10^(seq(log_first, log_last, length.out = 20)))
r_vals1 <- map_dbl(iter, ~median(r_squared(x[1:., , ])))
r_vals1_lower <- map_dbl(iter, ~quantile(r_squared(x[1:., , ]), 0.025))
r_vals1_upper <- map_dbl(iter, ~quantile(r_squared(x[1:., , ]), 0.975))

fit <- readRDS("../output/fit_alt_50000.rds")
x <- rstan::extract(fit, permuted=F, pars="x")
r_vals <- map_dbl(iter, ~median(r_squared(x[1:., , ])))
r_vals_lower <- map_dbl(iter, ~quantile(r_squared(x[1:., , ]), 0.025))
r_vals_upper <- map_dbl(iter, ~quantile(r_squared(x[1:., , ]), 0.975))
```

Compare nominal and alt parameterisations
```{r}
b_df <- 
  tibble(iteration=iter,
         middle=r_vals,
         lower=r_vals_lower,
         upper=r_vals_upper,
         variable="alt") %>% 
  bind_rows(tibble(iteration=iter,
         middle=r_vals1,
         lower=r_vals1_lower,
         upper=r_vals1_upper,
         variable="nom"))
# saveRDS(b_df, "../output/cauchy_r_squared.rds")
b_df <- readRDS("../output/cauchy_r_squared.rds")
g1 <- b_df %>% 
  ggplot(aes(x=iteration)) +
  geom_point(aes(y=middle,colour=as.factor(variable))) +
  geom_line(aes(y=middle,colour=as.factor(variable))) +
  ylim(NA, 1) +
  ylab(TeX("$R^2$")) +
  scale_color_grey("Parameterisation", labels=c("Alternative", "Nominal")) +
  xlab("Iteration") +
  theme(text=element_text(size=11, colour="black"),
        axis.text = element_text(size=10, colour="black"),
        legend.position = "none",
        axis.title.y = element_blank()) +
  scale_x_log10() +
  ggtitle(TeX("A. Quantile $R^2$"))
g1
```

Progression of R*
```{r}
fit <- readRDS("../output/fit_nominal_50000.rds")
x <- rstan::extract(fit, permuted=F, pars="x")

library(furrr)
future::plan(multiprocess)
laccuracy <- future_map_dbl(iter, function(i) r_star(x[1:i, , ]))

fit <- readRDS("../output/fit_alt_50000.rds")
x <- rstan::extract(fit, permuted=F, pars="x")

laccuracy1 <- future_map_dbl(iter, function(i) r_star(x[1:i, , ]))

a_df <- 
  tibble(iteration=iter,
         alt=laccuracy1,
       nom=laccuracy)
# saveRDS(a_df, "../output/cauchy_r_star.rds")
```

```{r}
a_df <- readRDS("../output/cauchy_r_star.rds")
g2 <- 
  a_df %>% 
  melt(id.vars="iteration") %>% 
  ggplot(aes(x=iteration, y=value, colour=as.factor(variable))) +
  geom_point() +
  geom_line() +
  ylab(TeX("$R^*$")) +
  scale_color_grey("Parameterisation", labels=c("Alternative", "Nominal")) +
  xlab("Iteration") +
  theme(text=element_text(size=11, colour="black"),
        axis.text = element_text(size=10, colour="black"),
        legend.position = c(0.8, 0.7),
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 1, linetype=2) +
  scale_x_log10() +
  scale_y_continuous(limits=c(0.8, NA), breaks = seq(1, 7, 1)) +
  ggtitle(TeX("D. Split-$R^*$"))
g2
```

Rhat
```{r}
r_hat_all_params <- function(x){
 nparams <- dim(x)[3] 
 rhat_vals <- map_dbl(seq(1, nparams, 1), ~rhat(split_chains(x[ , , .])))
 return(rhat_vals)
}

fit <- readRDS("../output/fit_nominal_50000.rds")
x <- rstan::extract(fit, permuted=F, pars="x")
future::plan(multiprocess)
r_hat <- future_map_dbl(iter, ~max(r_hat_all_params(x[1:., , ])))
r_hat_lower <- future_map_dbl(iter, ~quantile(r_hat_all_params(x[1:., , ]), 0.025))
r_hat_upper <- future_map_dbl(iter, ~quantile(r_hat_all_params(x[1:., , ]), 0.975))

fit1 <- readRDS("../output/fit_alt_50000.rds")
x <- rstan::extract(fit1, permuted=F, pars="x")
r_hat1 <- future_map_dbl(iter, ~max(r_hat_all_params(x[1:., , ])))
r_hat1_lower <- future_map_dbl(iter, ~quantile(r_hat_all_params(x[1:., , ]), 0.025))
r_hat1_upper <- future_map_dbl(iter, ~quantile(r_hat_all_params(x[1:., , ]), 0.975))


c_df <- 
  tibble(iteration=iter,
         middle=r_hat1,
         lower=r_hat1_lower,
         upper=r_hat1_upper,
         variable="alt") %>% 
  bind_rows(tibble(iteration=iter,
         middle=r_hat,
         lower=r_hat_lower,
         upper=r_hat_upper,
         variable="nom"))
# saveRDS(c_df, "../output/cauchy_r_hat.rds")
c_df <- readRDS("../output/cauchy_r_hat.rds")
g3 <- 
  c_df %>% 
  ggplot(aes(x=iteration)) +
  geom_point(aes(y=middle, colour=as.factor(variable))) +
  geom_line(aes(y=middle, colour=as.factor(variable))) +
  ylab(TeX("$\\hat{R}$")) +
  scale_color_grey("Parameterisation", labels=c("Alternative", "Nominal")) +
  scale_fill_grey("Parameterisation", labels=c("Alternative", "Nominal")) +
  xlab("Iteration") +
  theme(text=element_text(size=11, colour="black"),
        axis.text = element_text(size=10, colour="black"),
        legend.position = "none",
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 1, linetype=2) +
  scale_x_log10() +
  scale_y_continuous(limits=c(0.99, NA), breaks = seq(1, 6, 0.2)) +
  ggtitle(TeX("B. Max split-$\\hat{R}$"))
g3
```

Multivariate Rhat
```{r}
fit1 <- readRDS("../output/fit_alt_50000.rds")
x <- rstan::extract(fit1, permuted=F, pars="x")
Rhat_alt <- map_dbl(iter, ~r_hat_multivariate(x[1:., , ]))

fit1 <- readRDS("../output/fit_nominal_50000.rds")
x <- rstan::extract(fit1, permuted=F, pars="x")
Rhat_nom <- map_dbl(iter, ~r_hat_multivariate(x[1:., , ]))
g_df <- tibble(iteration=iter,
       alt=Rhat_alt,
       nom=Rhat_nom) 
g4 <- 
  tibble(iteration=iter,
       alt=Rhat_alt,
       nom=Rhat_nom) %>% 
  melt(id.vars="iteration") %>% 
  ggplot(aes(x=iteration)) +
  geom_point(aes(y=value, colour=as.factor(variable))) +
  geom_line(aes(y=value, colour=as.factor(variable))) +
  ylab(TeX("$\\hat{R}$")) +
  scale_color_grey("Parameterisation", labels=c("Alternative", "Nominal")) +
  scale_fill_grey("Parameterisation", labels=c("Alternative", "Nominal")) +
  xlab("Iteration") +
  theme(text=element_text(size=11, colour="black"),
        axis.text = element_text(size=10, colour="black"),
        legend.position = "none",
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 1, linetype=2) +
  scale_x_log10() +
  scale_y_continuous(limits=c(0.99, NA), breaks = seq(1, 6, 0.2)) +
  ggtitle(TeX("C. Multivariate split-$\\hat{R}$"))
```

Tail ESS
```{r}
fit1 <- readRDS("../output/fit_alt_50000.rds")
x <- rstan::extract(fit1, permuted=F, pars="x")
tail_ess_alt <- map_dbl(iter, function(i) min(map_dbl(seq(1, 50, 1), ~ess_tail(x[1:i, , .]))))
bulk_ess_alt <- map_dbl(iter, function(i) min(map_dbl(seq(1, 50, 1), ~ess_bulk(x[1:i, , .]))))

fit1 <- readRDS("../output/fit_nominal_50000.rds")
x <- rstan::extract(fit1, permuted=F, pars="x")
tail_ess_nom <- map_dbl(iter, function(i) min(map_dbl(seq(1, 50, 1), ~ess_tail(x[1:i, , .]))))
bulk_ess_nom <- map_dbl(iter, function(i) min(map_dbl(seq(1, 50, 1), ~ess_bulk(x[1:i, , .]))))

e_df <- tibble(iteration=iter,
               alt=tail_ess_alt,
               nom=tail_ess_nom)
saveRDS(e_df, "../output/cauchy_tail_ess.rds")
f_df <- tibble(iteration=iter,
               alt=bulk_ess_alt,
               nom=bulk_ess_nom)
saveRDS(f_df, "../output/cauchy_bulk_ess.rds")
e_df <- readRDS("../output/cauchy_tail_ess.rds")
f_df <- readRDS("../output/cauchy_bulk_ess.rds")
g5 <- 
  e_df %>% 
  melt(id.vars="iteration") %>% 
  ggplot(aes(x=iteration)) +
  geom_point(aes(y=value, colour=as.factor(variable))) +
  geom_line(aes(y=value, colour=as.factor(variable))) +
  scale_color_grey("Parameterisation", labels=c("Alternative", "Nominal")) +
  scale_fill_grey("Parameterisation", labels=c("Alternative", "Nominal")) +
  xlab("Iteration") +
  theme(text=element_text(size=11, colour="black"),
        axis.text = element_text(size=10, colour="black"),
        legend.position = "none",
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 400, linetype=2) +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle(TeX("E. Tail ESS"))

g6 <- 
  f_df %>% 
  melt(id.vars="iteration") %>% 
  ggplot(aes(x=iteration)) +
  geom_point(aes(y=value, colour=as.factor(variable))) +
  geom_line(aes(y=value, colour=as.factor(variable))) +
  scale_color_grey("Parameterisation", labels=c("Alternative", "Nominal")) +
  scale_fill_grey("Parameterisation", labels=c("Alternative", "Nominal")) +
  xlab("Iteration") +
  theme(text=element_text(size=11, colour="black"),
        axis.text = element_text(size=10, colour="black"),
        legend.position = "none",
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 400, linetype=2) +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle(TeX("F. Bulk ESS"))
```



Combine
```{r}
pdf("../output/cauchy_convergence.pdf", width = 12, height = 8)
multiplot(g1, g4, g5, g3, g2, g6, cols = 2)
dev.off()
```

Look at quantile accuracy for models converged by R* and Rhat
```{r}
fit <- readRDS("../output/fit_alt_50000.rds")
x <- rstan::extract(fit, permuted=F, pars="x")
quantile_errors <- function(x, lower=0.001){
  print(dim(x))
  qs <- seq(lower, 1-lower, lower)
  q_actual <- map_dbl(qs, ~qcauchy(.))
  errors <- matrix(nrow = length(qs),
                   ncol=dim(x)[3])
  for(i in 1:50){
    q_approx <- quantile(x[, , i], qs)
    errors[, i] <- abs(q_actual - q_approx)
  }
  errors <- errors %>% 
    as.data.frame() %>% 
    mutate(q=0.5-abs(0.5-qs)) %>% 
    melt(id.vars="q")
  return(errors)
}

errors_df <- quantile_errors(x, lower=0.001)
errors_df %>% 
  ggplot(aes(x=q, y=value)) +
  geom_jitter(width = 0.05) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(method="lm") +
  xlab("Distance from centre") +
  ylab("Error")

aLM <- lm(data=errors_df, formula="log(value)~log(q)")
summary(aLM)

errors_df <- quantile_errors(x[1:iter[8], , ], lower=0.001)
errors_df %>% 
  ggplot(aes(x=q, y=value)) +
  geom_jitter(width = 0.05) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(method="lm") +
  xlab("Distance from centre") +
  ylab("Error")

bLM <- lm(data=errors_df, formula="log(value)~log(q)")
summary(bLM)
```

